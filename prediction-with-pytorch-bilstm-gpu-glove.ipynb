{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.011865,
     "end_time": "2020-08-16T04:47:55.491668",
     "exception": false,
     "start_time": "2020-08-16T04:47:55.479803",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2020-08-16T04:47:55.521766Z",
     "iopub.status.busy": "2020-08-16T04:47:55.520910Z",
     "iopub.status.idle": "2020-08-16T04:47:55.529451Z",
     "shell.execute_reply": "2020-08-16T04:47:55.530255Z"
    },
    "papermill": {
     "duration": 0.026702,
     "end_time": "2020-08-16T04:47:55.530436",
     "exception": false,
     "start_time": "2020-08-16T04:47:55.503734",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/nlp-getting-started/test.csv\n",
      "/kaggle/input/nlp-getting-started/sample_submission.csv\n",
      "/kaggle/input/nlp-getting-started/train.csv\n",
      "/kaggle/input/glove100d/glove.6B.50d.txt\n",
      "/kaggle/input/glove100d/glove.6B.100d.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:4: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "import os\n",
    "import re\n",
    "\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "        \n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.009961,
     "end_time": "2020-08-16T04:47:55.551673",
     "exception": false,
     "start_time": "2020-08-16T04:47:55.541712",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## import inputs and randomize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2020-08-16T04:47:55.579252Z",
     "iopub.status.busy": "2020-08-16T04:47:55.578601Z",
     "iopub.status.idle": "2020-08-16T04:47:55.650910Z",
     "shell.execute_reply": "2020-08-16T04:47:55.651471Z"
    },
    "papermill": {
     "duration": 0.089323,
     "end_time": "2020-08-16T04:47:55.651602",
     "exception": false,
     "start_time": "2020-08-16T04:47:55.562279",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3228</th>\n",
       "      <td>4632</td>\n",
       "      <td>emergency%20services</td>\n",
       "      <td>Sydney, New South Wales</td>\n",
       "      <td>Goulburn man Henry Van Bilsen missing: Emergency services are searching for a Goulburn man who disappeared from hisÛ_ http://t.co/z99pKJzTRp</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3706</th>\n",
       "      <td>5271</td>\n",
       "      <td>fear</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The things we fear most in organizations--fluctuations disturbances imbalances--are the primary sources of creativity. - Margaret Wheatley</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6957</th>\n",
       "      <td>9982</td>\n",
       "      <td>tsunami</td>\n",
       "      <td>Land Of The Kings</td>\n",
       "      <td>@tsunami_esh ?? hey Esh</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2887</th>\n",
       "      <td>4149</td>\n",
       "      <td>drown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@POTUS you until you drown by water entering the lungs. You being alive has caused this great country to fall to shit because you're a pussy</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7464</th>\n",
       "      <td>10680</td>\n",
       "      <td>wounds</td>\n",
       "      <td>cody, austin follows ?*?</td>\n",
       "      <td>Crawling in my skin\\nThese wounds they will not hea</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id               keyword                  location  \\\n",
       "3228  4632   emergency%20services  Sydney, New South Wales    \n",
       "3706  5271   fear                  NaN                        \n",
       "6957  9982   tsunami               Land Of The Kings          \n",
       "2887  4149   drown                 NaN                        \n",
       "7464  10680  wounds                cody, austin follows ?*?   \n",
       "\n",
       "                                                                                                                                               text  \\\n",
       "3228  Goulburn man Henry Van Bilsen missing: Emergency services are searching for a Goulburn man who disappeared from hisÛ_ http://t.co/z99pKJzTRp   \n",
       "3706  The things we fear most in organizations--fluctuations disturbances imbalances--are the primary sources of creativity. - Margaret Wheatley      \n",
       "6957  @tsunami_esh ?? hey Esh                                                                                                                         \n",
       "2887  @POTUS you until you drown by water entering the lungs. You being alive has caused this great country to fall to shit because you're a pussy    \n",
       "7464  Crawling in my skin\\nThese wounds they will not hea                                                                                             \n",
       "\n",
       "      target  \n",
       "3228  1       \n",
       "3706  0       \n",
       "6957  0       \n",
       "2887  0       \n",
       "7464  1       "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\n",
    "test = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\n",
    "test = test.sample(frac=1,random_state = 1)\n",
    "train = train.sample(frac=1,random_state = 1)\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.010369,
     "end_time": "2020-08-16T04:47:55.673186",
     "exception": false,
     "start_time": "2020-08-16T04:47:55.662817",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### glove embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-16T04:47:55.701424Z",
     "iopub.status.busy": "2020-08-16T04:47:55.700747Z",
     "iopub.status.idle": "2020-08-16T04:48:19.561283Z",
     "shell.execute_reply": "2020-08-16T04:48:19.560530Z"
    },
    "papermill": {
     "duration": 23.876912,
     "end_time": "2020-08-16T04:48:19.561445",
     "exception": false,
     "start_time": "2020-08-16T04:47:55.684533",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "with open('/kaggle/input/glove100d/glove.6B.50d.txt','r',encoding = 'utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.array(values[1:]).astype(np.float)\n",
    "        embeddings_index[word] = coefs\n",
    "        \n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-16T04:48:19.603402Z",
     "iopub.status.busy": "2020-08-16T04:48:19.602512Z",
     "iopub.status.idle": "2020-08-16T04:48:19.630034Z",
     "shell.execute_reply": "2020-08-16T04:48:19.628772Z"
    },
    "papermill": {
     "duration": 0.05234,
     "end_time": "2020-08-16T04:48:19.630197",
     "exception": false,
     "start_time": "2020-08-16T04:48:19.577857",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 7613 entries, 3228 to 5157\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        7613 non-null   int64 \n",
      " 1   keyword   7552 non-null   object\n",
      " 2   location  5080 non-null   object\n",
      " 3   text      7613 non-null   object\n",
      " 4   target    7613 non-null   int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 356.9+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3263 entries, 1787 to 1061\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        3263 non-null   int64 \n",
      " 1   keyword   3237 non-null   object\n",
      " 2   location  2158 non-null   object\n",
      " 3   text      3263 non-null   object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 127.5+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(train.info())\n",
    "\n",
    "print(test.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.015424,
     "end_time": "2020-08-16T04:48:19.663691",
     "exception": false,
     "start_time": "2020-08-16T04:48:19.648267",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## drop unnecessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-16T04:48:19.710842Z",
     "iopub.status.busy": "2020-08-16T04:48:19.710067Z",
     "iopub.status.idle": "2020-08-16T04:48:19.716637Z",
     "shell.execute_reply": "2020-08-16T04:48:19.717587Z"
    },
    "papermill": {
     "duration": 0.038235,
     "end_time": "2020-08-16T04:48:19.717740",
     "exception": false,
     "start_time": "2020-08-16T04:48:19.679505",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 7613 entries, 3228 to 5157\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      7613 non-null   int64 \n",
      " 1   text    7613 non-null   object\n",
      " 2   target  7613 non-null   int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 237.9+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "train.drop(['keyword','location'],axis =1, inplace = True)\n",
    "\n",
    "print(train.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-16T04:48:19.763934Z",
     "iopub.status.busy": "2020-08-16T04:48:19.763032Z",
     "iopub.status.idle": "2020-08-16T04:48:19.770775Z",
     "shell.execute_reply": "2020-08-16T04:48:19.771505Z"
    },
    "papermill": {
     "duration": 0.036569,
     "end_time": "2020-08-16T04:48:19.771700",
     "exception": false,
     "start_time": "2020-08-16T04:48:19.735131",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3263 entries, 1787 to 1061\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      3263 non-null   int64 \n",
      " 1   text    3263 non-null   object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 76.5+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "test.drop(['keyword','location'], axis = 1, inplace = True)\n",
    "\n",
    "print(test.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.015893,
     "end_time": "2020-08-16T04:48:19.803867",
     "exception": false,
     "start_time": "2020-08-16T04:48:19.787974",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## clean the text column"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.016322,
     "end_time": "2020-08-16T04:48:19.836459",
     "exception": false,
     "start_time": "2020-08-16T04:48:19.820137",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "1. remove http links\n",
    "2. removing unkonwn characters \\x89U0 etc.\n",
    "3. remove #,@ =>\n",
    "4. remove special characters ',:;. etc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-16T04:48:19.875741Z",
     "iopub.status.busy": "2020-08-16T04:48:19.874834Z",
     "iopub.status.idle": "2020-08-16T04:48:19.881085Z",
     "shell.execute_reply": "2020-08-16T04:48:19.881777Z"
    },
    "papermill": {
     "duration": 0.0306,
     "end_time": "2020-08-16T04:48:19.881949",
     "exception": false,
     "start_time": "2020-08-16T04:48:19.851349",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    #2. remove unkonwn characrters\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "   \n",
    "    #1. remove http links\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    text = url.sub(r'',text)\n",
    "    \n",
    "    #3,4. remove #,@ and othet symbols\n",
    "    text = text.replace('#',' ')\n",
    "    text = text.replace('@',' ')\n",
    "    symbols = re.compile(r'[^A-Za-z0-9 ]')\n",
    "    text = symbols.sub(r'',text)\n",
    "    \n",
    "    #5. lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-16T04:48:19.929158Z",
     "iopub.status.busy": "2020-08-16T04:48:19.928220Z",
     "iopub.status.idle": "2020-08-16T04:48:20.220088Z",
     "shell.execute_reply": "2020-08-16T04:48:20.221407Z"
    },
    "papermill": {
     "duration": 0.323557,
     "end_time": "2020-08-16T04:48:20.221638",
     "exception": false,
     "start_time": "2020-08-16T04:48:19.898081",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train['text'] = train['text'].apply(lambda x: clean_text(x))\n",
    "test['text'] = test['text'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-16T04:48:20.266777Z",
     "iopub.status.busy": "2020-08-16T04:48:20.265881Z",
     "iopub.status.idle": "2020-08-16T04:48:20.276955Z",
     "shell.execute_reply": "2020-08-16T04:48:20.277616Z"
    },
    "papermill": {
     "duration": 0.038661,
     "end_time": "2020-08-16T04:48:20.277775",
     "exception": false,
     "start_time": "2020-08-16T04:48:20.239114",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3228</th>\n",
       "      <td>4632</td>\n",
       "      <td>goulburn man henry van bilsen missing emergency services are searching for a goulburn man who disappeared from his</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3706</th>\n",
       "      <td>5271</td>\n",
       "      <td>the things we fear most in organizationsfluctuations disturbances imbalancesare the primary sources of creativity  margaret wheatley</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6957</th>\n",
       "      <td>9982</td>\n",
       "      <td>tsunamiesh  hey esh</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2887</th>\n",
       "      <td>4149</td>\n",
       "      <td>potus you until you drown by water entering the lungs you being alive has caused this great country to fall to shit because youre a pussy</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7464</th>\n",
       "      <td>10680</td>\n",
       "      <td>crawling in my skinthese wounds they will not hea</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  \\\n",
       "3228  4632    \n",
       "3706  5271    \n",
       "6957  9982    \n",
       "2887  4149    \n",
       "7464  10680   \n",
       "\n",
       "                                                                                                                                            text  \\\n",
       "3228  goulburn man henry van bilsen missing emergency services are searching for a goulburn man who disappeared from his                           \n",
       "3706  the things we fear most in organizationsfluctuations disturbances imbalancesare the primary sources of creativity  margaret wheatley         \n",
       "6957   tsunamiesh  hey esh                                                                                                                         \n",
       "2887   potus you until you drown by water entering the lungs you being alive has caused this great country to fall to shit because youre a pussy   \n",
       "7464  crawling in my skinthese wounds they will not hea                                                                                            \n",
       "\n",
       "      target  \n",
       "3228  1       \n",
       "3706  0       \n",
       "6957  0       \n",
       "2887  0       \n",
       "7464  1       "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.011305,
     "end_time": "2020-08-16T04:48:20.304209",
     "exception": false,
     "start_time": "2020-08-16T04:48:20.292904",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## seperate id column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-16T04:48:20.337681Z",
     "iopub.status.busy": "2020-08-16T04:48:20.335949Z",
     "iopub.status.idle": "2020-08-16T04:48:20.338371Z",
     "shell.execute_reply": "2020-08-16T04:48:20.338848Z"
    },
    "papermill": {
     "duration": 0.023446,
     "end_time": "2020-08-16T04:48:20.338963",
     "exception": false,
     "start_time": "2020-08-16T04:48:20.315517",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_id = train['id']\n",
    "train.drop(['id'],axis=1, inplace = True)\n",
    "\n",
    "test_id = test['id']\n",
    "test.drop(['id'],axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.011394,
     "end_time": "2020-08-16T04:48:20.361888",
     "exception": false,
     "start_time": "2020-08-16T04:48:20.350494",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Word2idx and embedding dicts for LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-16T04:48:20.389281Z",
     "iopub.status.busy": "2020-08-16T04:48:20.388427Z",
     "iopub.status.idle": "2020-08-16T04:48:20.391512Z",
     "shell.execute_reply": "2020-08-16T04:48:20.391014Z"
    },
    "papermill": {
     "duration": 0.018449,
     "end_time": "2020-08-16T04:48:20.391608",
     "exception": false,
     "start_time": "2020-08-16T04:48:20.373159",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "word2idx = {}\n",
    "new_embedding_index = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.011214,
     "end_time": "2020-08-16T04:48:20.414453",
     "exception": false,
     "start_time": "2020-08-16T04:48:20.403239",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### instead of using all the 400000 word vectors lets use only vectors form words present in the train and test data. Next codes mean that we are giving each unique word a index(number) and storing in word2idx dictionary and also creating a new embedding dictionary which maps those numbers to a coeff from glove embeddings. If the word does not exist in the glove embedding then we give them a random coeffs of same dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-16T04:48:20.448327Z",
     "iopub.status.busy": "2020-08-16T04:48:20.444941Z",
     "iopub.status.idle": "2020-08-16T04:48:20.628905Z",
     "shell.execute_reply": "2020-08-16T04:48:20.628395Z"
    },
    "papermill": {
     "duration": 0.2032,
     "end_time": "2020-08-16T04:48:20.629020",
     "exception": false,
     "start_time": "2020-08-16T04:48:20.425820",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_X_list = []\n",
    "index = 1\n",
    "\n",
    "embed_keys = embeddings_index.keys()\n",
    "for x in train['text']:\n",
    "        list1 = x.split(' ')\n",
    "        new_list = []\n",
    "        for i in list1:\n",
    "            if((i in embed_keys)  and (i not in word2idx.keys())):\n",
    "                new_embedding_index[index] = embeddings_index[i]\n",
    "                word2idx[i] = index\n",
    "                new_list.append(index)\n",
    "                index=index+1   \n",
    "                \n",
    "            elif(i not in word2idx.keys()):\n",
    "                new_embedding_index[index] = np.random.normal(scale=0.4, size=(50, )).astype(np.float)\n",
    "                word2idx[i] = index\n",
    "                new_list.append(index)\n",
    "                index=index+1   \n",
    "\n",
    "            else:\n",
    "                new_list.append(word2idx[i])\n",
    "\n",
    "        train_X_list.append(new_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-16T04:48:20.665014Z",
     "iopub.status.busy": "2020-08-16T04:48:20.664071Z",
     "iopub.status.idle": "2020-08-16T04:48:20.793762Z",
     "shell.execute_reply": "2020-08-16T04:48:20.794246Z"
    },
    "papermill": {
     "duration": 0.153317,
     "end_time": "2020-08-16T04:48:20.794396",
     "exception": false,
     "start_time": "2020-08-16T04:48:20.641079",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_X_list = []\n",
    "index = len(word2idx)+1\n",
    "\n",
    "embed_keys = embeddings_index.keys()\n",
    "for x in test['text']:\n",
    "        list1 = x.split(' ')\n",
    "        new_list = []\n",
    "        for i in list1:\n",
    "            if((i in embed_keys)  and (i not in word2idx.keys())):\n",
    "                new_embedding_index[index] = embeddings_index[i]\n",
    "                word2idx[i] = index\n",
    "                new_list.append(index)\n",
    "                index=index+1   \n",
    "                \n",
    "            elif(i not in word2idx.keys()):\n",
    "                new_embedding_index[index] = np.random.normal(scale=0.4, size=(50, )).astype(np.float)\n",
    "                word2idx[i] = index\n",
    "                new_list.append(index)\n",
    "                index=index+1   \n",
    "\n",
    "            else:\n",
    "                new_list.append(word2idx[i])\n",
    "\n",
    "        test_X_list.append(new_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.012382,
     "end_time": "2020-08-16T04:48:20.819288",
     "exception": false,
     "start_time": "2020-08-16T04:48:20.806906",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### lest see total no of unique words in train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-16T04:48:20.848049Z",
     "iopub.status.busy": "2020-08-16T04:48:20.847370Z",
     "iopub.status.idle": "2020-08-16T04:48:20.851743Z",
     "shell.execute_reply": "2020-08-16T04:48:20.852225Z"
    },
    "papermill": {
     "duration": 0.020906,
     "end_time": "2020-08-16T04:48:20.852347",
     "exception": false,
     "start_time": "2020-08-16T04:48:20.831441",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22652\n"
     ]
    }
   ],
   "source": [
    "print(len(new_embedding_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.012187,
     "end_time": "2020-08-16T04:48:20.876551",
     "exception": false,
     "start_time": "2020-08-16T04:48:20.864364",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### its just 22652 unique words including words which may not be in the glove vector."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.01174,
     "end_time": "2020-08-16T04:48:20.900041",
     "exception": false,
     "start_time": "2020-08-16T04:48:20.888301",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.012442,
     "end_time": "2020-08-16T04:48:20.924410",
     "exception": false,
     "start_time": "2020-08-16T04:48:20.911968",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### first lets find the maximum length or no of words in a text column of train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-16T04:48:20.955738Z",
     "iopub.status.busy": "2020-08-16T04:48:20.954871Z",
     "iopub.status.idle": "2020-08-16T04:48:20.958848Z",
     "shell.execute_reply": "2020-08-16T04:48:20.958347Z"
    },
    "papermill": {
     "duration": 0.020968,
     "end_time": "2020-08-16T04:48:20.958962",
     "exception": false,
     "start_time": "2020-08-16T04:48:20.937994",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(map(len, train_X_list)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-16T04:48:20.988181Z",
     "iopub.status.busy": "2020-08-16T04:48:20.987351Z",
     "iopub.status.idle": "2020-08-16T04:48:20.991799Z",
     "shell.execute_reply": "2020-08-16T04:48:20.991251Z"
    },
    "papermill": {
     "duration": 0.020468,
     "end_time": "2020-08-16T04:48:20.991894",
     "exception": false,
     "start_time": "2020-08-16T04:48:20.971426",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(map(len, test_X_list)) "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.012699,
     "end_time": "2020-08-16T04:48:21.017008",
     "exception": false,
     "start_time": "2020-08-16T04:48:21.004309",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "1. #### so we will pad for length of 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-16T04:48:21.063151Z",
     "iopub.status.busy": "2020-08-16T04:48:21.062161Z",
     "iopub.status.idle": "2020-08-16T04:48:21.067609Z",
     "shell.execute_reply": "2020-08-16T04:48:21.068302Z"
    },
    "papermill": {
     "duration": 0.037606,
     "end_time": "2020-08-16T04:48:21.068465",
     "exception": false,
     "start_time": "2020-08-16T04:48:21.030859",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pad_features(reviews_int, seq_length):\n",
    "    features = np.zeros((len(reviews_int), seq_length), dtype = int)\n",
    "    for i, review in enumerate(reviews_int):\n",
    "        review_len = len(review)\n",
    "        \n",
    "        if review_len <= seq_length:\n",
    "            zeroes = list(np.zeros(seq_length-review_len))\n",
    "            new = zeroes+review \n",
    "        \n",
    "        elif review_len > seq_length:\n",
    "            new = review[0:seq_length]\n",
    "        \n",
    "        features[i,:] = np.array(new)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.017619,
     "end_time": "2020-08-16T04:48:21.109140",
     "exception": false,
     "start_time": "2020-08-16T04:48:21.091521",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### lest pad train and test lists"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.017768,
     "end_time": "2020-08-16T04:48:21.145479",
     "exception": false,
     "start_time": "2020-08-16T04:48:21.127711",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "train list total length is 7613. but for batch size we need 7616 elements so that all of them will go in batches or else last batch will be left over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-16T04:48:21.190620Z",
     "iopub.status.busy": "2020-08-16T04:48:21.189709Z",
     "iopub.status.idle": "2020-08-16T04:48:21.452129Z",
     "shell.execute_reply": "2020-08-16T04:48:21.451360Z"
    },
    "papermill": {
     "duration": 0.287515,
     "end_time": "2020-08-16T04:48:21.452335",
     "exception": false,
     "start_time": "2020-08-16T04:48:21.164820",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7616\n"
     ]
    }
   ],
   "source": [
    "train_X_list = pad_features(train_X_list,55)\n",
    "\n",
    "for i in range(3):\n",
    "    extra_list =[np.array(np.zeros(55).astype(int))]\n",
    "    train_X_list =  np.append(train_X_list,extra_list, axis=0)\n",
    "    \n",
    "print(len(train_X_list))   "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.018231,
     "end_time": "2020-08-16T04:48:21.489329",
     "exception": false,
     "start_time": "2020-08-16T04:48:21.471098",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "As we added three new rows in train_X_list we need to add three new rows in train_y_list. all three will be zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-16T04:48:21.539386Z",
     "iopub.status.busy": "2020-08-16T04:48:21.533588Z",
     "iopub.status.idle": "2020-08-16T04:48:21.546193Z",
     "shell.execute_reply": "2020-08-16T04:48:21.546914Z"
    },
    "papermill": {
     "duration": 0.039524,
     "end_time": "2020-08-16T04:48:21.547090",
     "exception": false,
     "start_time": "2020-08-16T04:48:21.507566",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7616\n"
     ]
    }
   ],
   "source": [
    "train_y_list=[]\n",
    "for i in train['target']:\n",
    "    train_y_list.append(i)\n",
    "    \n",
    "for i in range(3):\n",
    "    train_y_list.append(0)\n",
    "print(len(train_y_list))\n",
    "\n",
    "train_y_list=np.array(train_y_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.017335,
     "end_time": "2020-08-16T04:48:21.582368",
     "exception": false,
     "start_time": "2020-08-16T04:48:21.565033",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Here the test_X_list has 3263 elements i am considering 64 as batchsize. so in the training or testing it happens in batches format, but in the test set last 63 elements will be left over because of batch size. so to make them also to go through the LSTM i am adding  another row in test list which contains only zeors. so that 3264 can be divided by 64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-16T04:48:21.628130Z",
     "iopub.status.busy": "2020-08-16T04:48:21.627260Z",
     "iopub.status.idle": "2020-08-16T04:48:21.751628Z",
     "shell.execute_reply": "2020-08-16T04:48:21.752696Z"
    },
    "papermill": {
     "duration": 0.153055,
     "end_time": "2020-08-16T04:48:21.752876",
     "exception": false,
     "start_time": "2020-08-16T04:48:21.599821",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_X_list = pad_features(test_X_list,55)\n",
    "\n",
    "\n",
    "extra_list =[np.array(np.zeros(55).astype(int))]\n",
    "\n",
    "\n",
    "test_X_list =  np.append(test_X_list,extra_list, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.016575,
     "end_time": "2020-08-16T04:48:21.786684",
     "exception": false,
     "start_time": "2020-08-16T04:48:21.770109",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "embedding dictionary starts with 1 so at 0 index nothing will be there. i am placing all zeors in 0 index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-16T04:48:21.828225Z",
     "iopub.status.busy": "2020-08-16T04:48:21.827402Z",
     "iopub.status.idle": "2020-08-16T04:48:21.829529Z",
     "shell.execute_reply": "2020-08-16T04:48:21.828903Z"
    },
    "papermill": {
     "duration": 0.026328,
     "end_time": "2020-08-16T04:48:21.829636",
     "exception": false,
     "start_time": "2020-08-16T04:48:21.803308",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_embedding_index[0] = np.array(np.zeros(50)).astype(np.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.019904,
     "end_time": "2020-08-16T04:48:21.865324",
     "exception": false,
     "start_time": "2020-08-16T04:48:21.845420",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data loading and batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-16T04:48:21.907013Z",
     "iopub.status.busy": "2020-08-16T04:48:21.906392Z",
     "iopub.status.idle": "2020-08-16T04:48:23.357030Z",
     "shell.execute_reply": "2020-08-16T04:48:23.357587Z"
    },
    "papermill": {
     "duration": 1.473955,
     "end_time": "2020-08-16T04:48:23.357755",
     "exception": false,
     "start_time": "2020-08-16T04:48:21.883800",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "train_data = TensorDataset(torch.from_numpy(train_X_list),torch.from_numpy(train_y_list))\n",
    "\n",
    "\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_data, batch_size = batch_size, drop_last = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.012748,
     "end_time": "2020-08-16T04:48:23.383465",
     "exception": false,
     "start_time": "2020-08-16T04:48:23.370717",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## LSTM model with Pytorch utilizes GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-16T04:48:23.431262Z",
     "iopub.status.busy": "2020-08-16T04:48:23.429264Z",
     "iopub.status.idle": "2020-08-16T04:48:23.431969Z",
     "shell.execute_reply": "2020-08-16T04:48:23.432464Z"
    },
    "papermill": {
     "duration": 0.035992,
     "end_time": "2020-08-16T04:48:23.432591",
     "exception": false,
     "start_time": "2020-08-16T04:48:23.396599",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class BiLSTM(nn.Module):\n",
    "    #rnn for sentiment analysis\n",
    "    \n",
    "    def __init__(self,weights_matrix, output_size, hidden_dim,hidden_dim2, n_layers, drop_prob=0.5):\n",
    "        #initialize model by setting up the layers\n",
    "        super(BiLSTM, self).__init__()\n",
    "        \n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        #embedding and lstm layers and embedding from the glove\n",
    "        num_embeddings, embedding_dim = weights_matrix.shape\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "       \n",
    "        #getting values or parameters for embedding layer \n",
    "        self.embedding.weight = nn.Parameter(weights_matrix)\n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_dim,hidden_dim, n_layers, dropout = drop_prob, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        \n",
    "        #dropoutlayer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        #linear and sigmoid layers\n",
    "        self.fullyconnect1 = nn.Linear(hidden_dim,hidden_dim2)\n",
    "        \n",
    "        self.fullyconnect2 = nn.Linear(hidden_dim2, output_size)\n",
    "\n",
    "        #self.fullyconnect3 = nn.Linear(hidden_dim3, output_size)\n",
    "        \n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        #forward pass of our model \n",
    "        batch_size = x.size(0)\n",
    "         \n",
    "        #embedding and lstm out\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_outs, hidden = self.lstm(embeds, hidden)\n",
    "        \n",
    "        # stack up lstm outputs\n",
    "        lstm_outs = lstm_outs.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        \n",
    "        #dropout and fully connected layer\n",
    "        out = self.dropout(lstm_outs)\n",
    "        out = self.fullyconnect1(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fullyconnect2(out)\n",
    "        #sigmoid function\n",
    "        sig_out = self.sig(out)\n",
    "        \n",
    "         # reshape to be batch_size first\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "        sig_out = sig_out[:, -1] # get last batch of labels\n",
    "        \n",
    "        #return last sigmoid output and hidden state\n",
    "        return sig_out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size,train_on_gpu=False):\n",
    "        # initialize hidden state\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "            \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers*2, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.n_layers*2, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers*2, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers*2, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.012358,
     "end_time": "2020-08-16T04:48:23.457952",
     "exception": false,
     "start_time": "2020-08-16T04:48:23.445594",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### initialize bilstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-16T04:48:23.493918Z",
     "iopub.status.busy": "2020-08-16T04:48:23.493068Z",
     "iopub.status.idle": "2020-08-16T04:48:23.550037Z",
     "shell.execute_reply": "2020-08-16T04:48:23.550597Z"
    },
    "papermill": {
     "duration": 0.08032,
     "end_time": "2020-08-16T04:48:23.550761",
     "exception": false,
     "start_time": "2020-08-16T04:48:23.470441",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiLSTM(\n",
      "  (embedding): Embedding(22653, 50)\n",
      "  (lstm): LSTM(50, 200, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fullyconnect1): Linear(in_features=200, out_features=50, bias=True)\n",
      "  (fullyconnect2): Linear(in_features=50, out_features=1, bias=True)\n",
      "  (sig): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "vals = np.array(list(new_embedding_index.values()))\n",
    "vals = torch.from_numpy(vals)\n",
    "\n",
    "output_size = 1\n",
    "hidden_dim = 200\n",
    "hidden_dim2 = 50\n",
    "#hidden_dim3 = 50\n",
    "n_layers = 2\n",
    "\n",
    "net = BiLSTM(vals, output_size, hidden_dim,hidden_dim2, n_layers)\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-16T04:48:23.582628Z",
     "iopub.status.busy": "2020-08-16T04:48:23.580783Z",
     "iopub.status.idle": "2020-08-16T04:48:23.583324Z",
     "shell.execute_reply": "2020-08-16T04:48:23.583798Z"
    },
    "papermill": {
     "duration": 0.019868,
     "end_time": "2020-08-16T04:48:23.583910",
     "exception": false,
     "start_time": "2020-08-16T04:48:23.564042",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_on_gpu = True"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.012395,
     "end_time": "2020-08-16T04:48:23.608986",
     "exception": false,
     "start_time": "2020-08-16T04:48:23.596591",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-16T04:48:23.648809Z",
     "iopub.status.busy": "2020-08-16T04:48:23.648085Z",
     "iopub.status.idle": "2020-08-16T04:49:56.782887Z",
     "shell.execute_reply": "2020-08-16T04:49:56.782184Z"
    },
    "papermill": {
     "duration": 93.161298,
     "end_time": "2020-08-16T04:49:56.783045",
     "exception": false,
     "start_time": "2020-08-16T04:48:23.621747",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/12... Loss: 0.437527...\n",
      "Epoch: 2/12... Loss: 0.348406...\n",
      "Epoch: 3/12... Loss: 0.207398...\n",
      "Epoch: 4/12... Loss: 0.141215...\n",
      "Epoch: 5/12... Loss: 0.072095...\n",
      "Epoch: 6/12... Loss: 0.107385...\n",
      "Epoch: 7/12... Loss: 0.065853...\n",
      "Epoch: 8/12... Loss: 0.046517...\n",
      "Epoch: 9/12... Loss: 0.069189...\n",
      "Epoch: 10/12... Loss: 0.068173...\n",
      "Epoch: 11/12... Loss: 0.027111...\n",
      "Epoch: 12/12... Loss: 0.089525...\n"
     ]
    }
   ],
   "source": [
    "lr=0.001\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "# training params\n",
    "\n",
    "epochs =12\n",
    "\n",
    "counter = 0\n",
    "print_every = 64\n",
    "clip=5 # gradient clipping\n",
    "\n",
    "net = net.float()\n",
    "# move model to GPU, if available\n",
    "if(train_on_gpu):\n",
    "    net.cuda()\n",
    "\n",
    "net.train()\n",
    "# train for some number of epochs\n",
    "for e in range(epochs):\n",
    "    # initialize hidden state\n",
    "    h = net.init_hidden(batch_size)\n",
    "\n",
    "    # batch loop\n",
    "    for inputs, labels in train_loader:\n",
    "        counter += 1\n",
    "\n",
    "\n",
    "        # Creating new variables for the hidden state, otherwise\n",
    "        # we'd backprop through the entire training history\n",
    "        h = tuple([each.data for each in h])\n",
    "\n",
    "        # zero accumulated gradients\n",
    "        net.zero_grad()\n",
    "\n",
    "        # get the output from the model\n",
    "        inputs = inputs.type(torch.LongTensor)\n",
    "        inputs = inputs.cuda() \n",
    "        labels = labels.cuda()\n",
    "        output, h = net(inputs, h)\n",
    "        # calculate the loss and perform backprop\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        # loss stats\n",
    "       \n",
    "    print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                \"Loss: {:.6f}...\".format(loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.013535,
     "end_time": "2020-08-16T04:49:56.810548",
     "exception": false,
     "start_time": "2020-08-16T04:49:56.797013",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-16T04:49:56.846733Z",
     "iopub.status.busy": "2020-08-16T04:49:56.845887Z",
     "iopub.status.idle": "2020-08-16T04:49:57.610104Z",
     "shell.execute_reply": "2020-08-16T04:49:57.609468Z"
    },
    "papermill": {
     "duration": 0.785923,
     "end_time": "2020-08-16T04:49:57.610251",
     "exception": false,
     "start_time": "2020-08-16T04:49:56.824328",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "test_data = torch.from_numpy(test_X_list)\n",
    "\n",
    "test_loader = DataLoader(test_data,batch_size=batch_size)\n",
    "\n",
    "h = net.init_hidden(batch_size)\n",
    "\n",
    "pred = []\n",
    "\n",
    "net.eval()\n",
    "# iterate over test data\n",
    "for inputs in test_loader:\n",
    "\n",
    "    # Creating new variables for the hidden state, otherwise\n",
    "    # we'd backprop through the entire training history\n",
    "    h = tuple([each.data for each in h])\n",
    "    \n",
    "    # get predicted outputs\n",
    "    inputs = inputs.type(torch.LongTensor)\n",
    "    if(train_on_gpu):\n",
    "        inputs = inputs.cuda()\n",
    "        \n",
    "    output, h = net(inputs, h)\n",
    "    \n",
    "    # convert output probabilities to predicted class (0 or 1)\n",
    "    pred.append(torch.round(output.squeeze()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-16T04:49:57.644309Z",
     "iopub.status.busy": "2020-08-16T04:49:57.643665Z",
     "iopub.status.idle": "2020-08-16T04:49:57.659222Z",
     "shell.execute_reply": "2020-08-16T04:49:57.659905Z"
    },
    "papermill": {
     "duration": 0.035646,
     "end_time": "2020-08-16T04:49:57.660057",
     "exception": false,
     "start_time": "2020-08-16T04:49:57.624411",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3263\n"
     ]
    }
   ],
   "source": [
    "prediction = []\n",
    "for i in pred:\n",
    "    prediction.append(i.tolist())\n",
    "\n",
    "pred = []\n",
    "\n",
    "pred = [item for sublist in prediction for item in sublist]\n",
    "\n",
    "pred = pred[:-1] # because in test we added extra row in the last for batch size matching.\n",
    "\n",
    "pred = [int(i) for i in pred]\n",
    "print(len(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-16T04:49:57.696558Z",
     "iopub.status.busy": "2020-08-16T04:49:57.695849Z",
     "iopub.status.idle": "2020-08-16T04:49:57.794650Z",
     "shell.execute_reply": "2020-08-16T04:49:57.793540Z"
    },
    "papermill": {
     "duration": 0.12056,
     "end_time": "2020-08-16T04:49:57.794781",
     "exception": false,
     "start_time": "2020-08-16T04:49:57.674221",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "output = pd.DataFrame({'id': test_id,'target': pred})\n",
    "\n",
    "output.sort_values([\"id\"], axis=0, \n",
    "                 ascending=True, inplace=True)\n",
    "\n",
    "output.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 128.04311,
   "end_time": "2020-08-16T04:49:59.087396",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-08-16T04:47:51.044286",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
